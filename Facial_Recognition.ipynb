{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJjjq3bHxGKm"
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python tensorflow tensorflow-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774
    },
    "executionInfo": {
     "elapsed": 984746,
     "status": "error",
     "timestamp": 1731601582415,
     "user": {
      "displayName": "Erick Vinicius Reboucas Cruz",
      "userId": "17049041941800880523"
     },
     "user_tz": 180
    },
    "id": "fLl4smHPmCpM",
    "outputId": "11884bdb-7cf7-4513-b4e0-7cbc379ba778"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tensorflow  as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ... rest of your code ...\n",
    "\n",
    "# Segundo a pesquisa a biblioteca Face Recognitio (Dlib)  a mmelhro par o projeto\n",
    "\n",
    "\n",
    "test_image = 'class.jpg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SignatureMap({})\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Caminho para o diretório onde está o modelo\n",
    "model_dir = 'model'\n",
    "\n",
    "# Carregar o modelo\n",
    "model = tf.saved_model.load(model_dir)\n",
    "\n",
    "# Verifique a assinatura do modelo\n",
    "print(model.signatures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 4620,
     "status": "error",
     "timestamp": 1731590436444,
     "user": {
      "displayName": "Erick Vinicius Reboucas Cruz",
      "userId": "17049041941800880523"
     },
     "user_tz": 180
    },
    "id": "5TamHmBauQ6k",
    "outputId": "e29aefd0-2b8c-4938-eba5-00c1d4c86eeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Erick\\AppData\\Local\\Temp\\tmpuq12jejc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Erick\\AppData\\Local\\Temp\\tmpuq12jejc\\assets\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "Could not translate MLIR to FlatBuffer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n\u001b[1;32m----> 2\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Salvar o modelo TensorFlow Lite\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelo_reconhecimento_facial.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1231\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1230\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1231\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1183\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[0;32m   1182\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m-> 1183\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1184\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1744\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m \n\u001b[0;32m   1735\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;124;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1744\u001b[0m   saved_model_convert_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[0;32m   1746\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1725\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1721\u001b[0m   graph_def, input_tensors, output_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1722\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_keras_to_saved_model(temp_dir)\n\u001b[0;32m   1723\u001b[0m   )\n\u001b[0;32m   1724\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_dir:\n\u001b[1;32m-> 1725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFLiteKerasModelConverterV2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1729\u001b[0m   shutil\u001b[38;5;241m.\u001b[39mrmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1466\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[1;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[0;32m   1459\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1460\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing new converter: If you encounter a problem \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1461\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease file a bug. You can opt-out \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1462\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby setting experimental_new_converter=False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[1;32m-> 1466\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_graphdef\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconverter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_tflite_model(\n\u001b[0;32m   1474\u001b[0m     result,\n\u001b[0;32m   1475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quant_mode,\n\u001b[0;32m   1476\u001b[0m     _build_conversion_flags(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs)\u001b[38;5;241m.\u001b[39mdebug_options,\n\u001b[0;32m   1477\u001b[0m     quant_io\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer,\n\u001b[0;32m   1478\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:212\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     report_error_message(\u001b[38;5;28mstr\u001b[39m(converter_error))\n\u001b[1;32m--> 212\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m converter_error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Re-throws the exception.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:1014\u001b[0m, in \u001b[0;36mconvert_graphdef\u001b[1;34m(input_data, input_tensors, output_tensors, **kwargs)\u001b[0m\n\u001b[0;32m   1011\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m     model_flags\u001b[38;5;241m.\u001b[39moutput_arrays\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mget_tensor_name(output_tensor))\n\u001b[1;32m-> 1014\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversion_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug_info_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_mlir_converter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_mlir_converter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Erick\\anaconda3\\envs\\Study\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:376\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[0;32m    368\u001b[0m         conversion_flags\u001b[38;5;241m.\u001b[39mguarantee_all_funcs_one_use \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m convert(\n\u001b[0;32m    370\u001b[0m             model_flags,\n\u001b[0;32m    371\u001b[0m             conversion_flags,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m             enable_mlir_converter,\n\u001b[0;32m    375\u001b[0m         )\n\u001b[1;32m--> 376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converter_error\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _run_deprecated_conversion_binary(\n\u001b[0;32m    379\u001b[0m     model_flags\u001b[38;5;241m.\u001b[39mSerializeToString(),\n\u001b[0;32m    380\u001b[0m     conversion_flags\u001b[38;5;241m.\u001b[39mSerializeToString(),\n\u001b[0;32m    381\u001b[0m     input_data_str,\n\u001b[0;32m    382\u001b[0m     debug_info_str,\n\u001b[0;32m    383\u001b[0m )\n",
      "\u001b[1;31mConverterError\u001b[0m: Could not translate MLIR to FlatBuffer."
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Salvar o modelo TensorFlow Lite\n",
    "with open(\"modelo_reconhecimento_facial.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Inicializar MediaPipe para detecção de rostos\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Carregar o modelo TensorFlow Lite\n",
    "interpreter = tf.lite.Interpreter(model_path=\"seu_modelo_de_embeddings.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Função para obter embeddings\n",
    "def extrair_embedding(face_image):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_data = np.expand_dims(face_image, axis=0).astype(np.float32) / 255.0\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    return interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "# Função principal para detecção de rostos e extração de embeddings\n",
    "def reconhecer_presenca(imagem):\n",
    "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        imagem_rgb = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "        resultados = face_detection.process(imagem_rgb)\n",
    "\n",
    "        embeddings_faces = []\n",
    "        if resultados.detections:\n",
    "            for deteccao in resultados.detections:\n",
    "                bbox = deteccao.location_data.relative_bounding_box\n",
    "                h, w, _ = imagem.shape\n",
    "                x, y, largura, altura = int(bbox.xmin * w), int(bbox.ymin * h), int(bbox.width * w), int(bbox.height * h)\n",
    "                face = imagem[y:y + altura, x:x + largura]\n",
    "                face_resized = cv2.resize(face, (160, 160))  # Ajuste o tamanho conforme o modelo\n",
    "                embeddings_faces.append(extrair_embedding(face_resized))\n",
    "\n",
    "        return embeddings_faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = [test_image]\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print and draw face mesh landmarks on the image.\n",
    "    if not results.multi_face_landmarks:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "      # print('face_landmarks:', face_landmarks)\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_tesselation_style())\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_contours_style())\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "      \n",
    "    cv2.imwrite('annotated_image' + str(idx) + '.png', annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reconhecer_presenca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m imagem \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(test_image)\n\u001b[1;32m----> 2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mreconhecer_presenca\u001b[49m(imagem)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings extraídos:\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reconhecer_presenca' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "imagem = cv2.imread(test_image)\n",
    "embeddings = reconhecer_presenca(imagem)\n",
    "print(\"Embeddings extraídos:\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oB__RzST-fX1"
   },
   "outputs": [],
   "source": [
    "# Ler https://docs.ultralytics.com/quickstart/#use-ultralytics-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VppXAJB8y9U-"
   },
   "source": [
    "# Testando eficiência de cada modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1xPrOdZvsQ8"
   },
   "outputs": [],
   "source": [
    "#Modelo v8s\n",
    "model = YOLO(\"yolov8s.pt\") # load the model\n",
    "\n",
    "\n",
    "#results = model.train(data=\"coco128.yaml\", epochs=5)\n",
    "\n",
    "results = model(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lVcSa1eG5zt"
   },
   "outputs": [],
   "source": [
    "# Usando o Modelo 11\n",
    "model = YOLO(\"yolo11n.pt\")  # load an official model\n",
    "#model = YOLO(\"path/to/best.pt\")  # load a custom model\n",
    "\n",
    "# Predict with the model\n",
    "results = model(test_image)  #predict on an image\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhWDeG_0GSj2"
   },
   "outputs": [],
   "source": [
    "# Teste para ver se reconhece uma escova de dentes\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")  # load an official model\n",
    "\n",
    "#results = model(\"./tooth.png\")  # predict on an image\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zCu296GysCv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSDLnRHtysMX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShEy_k9wytq2"
   },
   "source": [
    "# Detecção de pessoas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O89Cr8N48djB"
   },
   "outputs": [],
   "source": [
    "# importing the cv2 library\n",
    "import cv2\n",
    "\n",
    "# loading the haar case algorithm file into alg variable\n",
    "alg = \"/content/drive/MyDrive/Colab Notebooks/haarcascade_frontalface_default.xml\"\n",
    "# passing the algorithm to OpenCV\n",
    "haar_cascade = cv2.CascadeClassifier(alg)\n",
    "# reading the image\n",
    "img = cv2.imread(test_image, 0)\n",
    "# creating a black and white version of the image\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "# detecting the faces\n",
    "faces = haar_cascade.detectMultiScale(\n",
    "    gray_img, scaleFactor=1.05, minNeighbors=2, minSize=(100, 100)\n",
    ")\n",
    "\n",
    "i = 0\n",
    "# for each face detected\n",
    "for x, y, w, h in faces:\n",
    "    # crop the image to select only the face\n",
    "    cropped_image = img[y : y + h, x : x + w]\n",
    "    # loading the target image path into target_file_name variable  - replace <INSERT YOUR TARGET IMAGE NAME HERE> with the path to your target image\n",
    "    target_file_name = 'stored-faces/' + str(i) + '.jpg'\n",
    "    cv2.imwrite(\n",
    "        target_file_name,\n",
    "        cropped_image,\n",
    "    )\n",
    "    i = i + 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3KxGwFG8rGz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# connecting to the database - replace the SERVICE URI with the service URI\n",
    "conn = psycopg2.connect(\"<SERVICE_URI>\")\n",
    "\n",
    "for filename in os.listdir(\"stored-faces\"):\n",
    "    # opening the image\n",
    "    img = Image.open(\"stored-faces/\" + filename)\n",
    "    # loading the `imgbeddings`\n",
    "    ibed = imgbeddings()\n",
    "    # calculating the embeddings\n",
    "    embedding = ibed.to_embeddings(img)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO pictures values (%s,%s)\", (filename, embedding[0].tolist()))\n",
    "    print(filename)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7aEUwy2y1V3"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# Carrega a imagem usando OpenCV\n",
    "img = cv2.imread(test_image)\n",
    "results = model(img)\n",
    "\n",
    "after_img = results[0].plot()  # Desenha as caixas na imagem original\n",
    "plt.imshow(cv2.cvtColor(after_img, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Remove os eixos\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zfx_YNX2wRhM"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Lista de imagens de referência\n",
    "reference_images = [\n",
    "    ('/content/drive/MyDrive/Colab Notebooks/Erick', 'Erick'),\n",
    "    ('/content/drive/MyDrive/Colab Notebooks/Plank', 'Amigo'),\n",
    "    ('/content/drive/MyDrive/Colab Notebooks/Einstein', 'Mule')\n",
    "]\n",
    "\n",
    "# Verifica se a imagem foi carregada corretamente\n",
    "if img is None:\n",
    "    print(\"Erro ao carregar a imagem de teste.\")\n",
    "else:\n",
    "    face_found = False\n",
    "\n",
    "    # Loop para comparar a imagem de teste com as imagens de referência\n",
    "    for path, name in reference_images:\n",
    "        try:\n",
    "            # Realiza a busca utilizando DeepFace\n",
    "            result = DeepFace.find(img_path=test_image, db_path=path, model_name='VGG-Face')\n",
    "\n",
    "            # Verifique o formato do retorno\n",
    "            if not result:\n",
    "                print(f\"Nenhum resultado encontrado no diretório {path}.\")\n",
    "                continue\n",
    "\n",
    "            # Iterate through the results and check if a match is found\n",
    "            for item in result:\n",
    "                # Aqui, pode ser necessário ajustar a verificação dependendo do formato do retorno\n",
    "                if 'verified' in item and item['verified'] is True:\n",
    "                    face_found = True\n",
    "                    print(f'Rosto de {name} encontrado na imagem.')\n",
    "                    break  # Sai do loop interno se uma correspondência for encontrada\n",
    "\n",
    "            if face_found:\n",
    "                break  # Sai do loop externo se uma correspondência for encontrada\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar a pasta {path}: {e}\")\n",
    "\n",
    "    if not face_found:\n",
    "        print(\"Nenhum rosto conhecido foi identificado na imagem.\")\n",
    "\n",
    "    # Exibe a imagem analisada\n",
    "    #img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSFoI91T1uAk"
   },
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "# Caminho da imagem de teste\n",
    "img = '/content/drive/MyDrive/Colab Notebooks/20241113_154643.jpg'  # Substitua pelo caminho correto da imagem de teste\n",
    "\n",
    "# Caminho da imagem de referência (imagem de Erick ou qualquer outra pessoa)\n",
    "db = '/content/drive/MyDrive/Colab Notebooks/Erick/image.png'  # Substitua pela imagem de referência\n",
    "\n",
    "# Realiza a verificação entre a imagem de teste e a referência\n",
    "result = DeepFace.verify(img1_path=img, img2_path=db, model_name='VGG-Face')\n",
    "\n",
    "# Exibe o resultado\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMh5JdLiNJeT+j9iNcRbo+V",
   "mount_file_id": "1xDw95JFdV0Fk5uT9ZLwrJNlaiWhGzNNQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
